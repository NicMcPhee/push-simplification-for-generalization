\section{Introduction}
\label{sec:intro}

\todo[inline]{Motivation and overview.}

Things we need to do somewhere in this paper:
\begin{itemize}
	\item Convince everyone that this is awesome. It would be particularly cool if we can somehow make the case that these ideas are relevant even if people aren't doing stack based work. Can we do that, though? Most (i.e., tree-based)
	GP systems don't allow for things like silencing of nodes or replacing nodes
	with NOOPs because it typically wouldn't be clear how to interpret such a tree.
	\item Cover relevant work. This might require some digging?
	\item Cover relevant background. This is going to (at least) require explaining PLUSH genomes, the conversion from PLUSH to PUSH, the evaluation of PUSH programs (so we can understand what NOOPs do), and silent genes.
	\item Describe the 5 simplification methods
	\item Experimental setup
	\item Results
	\item Discussion
	\item Future work
	\item Conclude
\end{itemize}

Smaller solutions have other benefits as well: understandability, faster runtimes, etc. But, we won't concentrate on those in this paper.

\section{PUSH, PLUSH, etc.}
\label{sec:push}

\todo[inline]{Explain necessary aspects of PUSH, PLUSH, and the like. Include talk of silencing genes, and how a silenced gene does not affect the resulting program, including instructions and closing parentheses.}

\todo[inline]{We should briefly define what the ``size'' of a push program is, since we discuss size a lot later.}

\section{Automatic Simplification in Push}
\label{sec:simplification}

Automatic simplification was introduced to Push as a tool for making evolved programs easier to understand \cite{Robinson:2001:GPtieus, spector:2002:GPEM}. While it can be applied to any program, it has typically been used post-run to make solution programs easier to understand without changing their behavior \cite{Spector:2014:GECCOcomp}. It has also been used as a bloat-control genetic operator during genetic programming runs, with mixed success \cite{Zhan:2014:GECCOcomp}. \todo[inline]{Might make more sense to move the above paragraph to the intro?}

Automatic simplification is a simple hill-climbing algorithm that tries to make an individual's program smaller without changing the program's behavior on the training cases. At each step, we first make small changes to the program to make it smaller. We then check whether the smaller program produces the same error vector as the original program. If so, we continue with the new program; otherwise, we reverts to the previous program. This process repeats for a set number of simplification steps, and then returns the resulting simplified program. The algorithm is presented in detail in Algorithm \ref{alg:simplification}.

\begin{algorithm}[ht]
\caption{Automatic Simplification}
\label{alg:simplification}

\SetKwInOut{Input}{Input}
\SetKwData{ErrorVector}{errorVector}
\SetKwData{NewErrorVector}{newErrorVector}
\SetKwData{Ind}{ind}
\SetKwData{NewInd}{newInd}
\SetKwData{Steps}{steps}
\SetKwData{S}{s}
\SetKwFunction{ComputeErrors}{computeErrorVector}
\SetKwFunction{TakeStep}{takeSimplificationStep}

\Input{individual \Ind, number of simplification \Steps, method for simplification step \TakeStep}
\BlankLine

\ErrorVector $\leftarrow$ \ComputeErrors{\Ind} \;
\For{\S $\leftarrow 0$ \KwTo \Steps}{

	\NewInd $\leftarrow$ \TakeStep{\Ind} \;
	\NewErrorVector $\leftarrow$ \ComputeErrors{\NewInd} \;
	\If{\NewErrorVector $=$ \ErrorVector}{
		\Ind $\leftarrow$ \NewInd
	}
}
\Return \Ind
\end{algorithm}

In this paper we explore five different methods of automatic simplification. Each method follows the same general algorithm; they only differ in the particular steps taken to make programs smaller. %The five methods are listed in Table ???, along with a description of what they do at each simplification step.
Below is a description of each method:

\textbf{Program} simplification, which has been used in Push since its inception \cite{Robinson:2001:GPtieus}, acts on an individual's Push program, as opposed to its linear Plush genome. At each simplification step, it has an 80\% chance of removing one or two random "code points" from the program, where a \textit{code point} is an instruction, a constant, or a parenthesized block of code. The other 20\% of the time it randomly removes one set of parentheses from a code block in the program, flattening the code inside the parentheses into the level that the code block occupied.

The other four simplification methods act on an individual's linear Plush genome prior to its translation into a Push program. There are three basic steps that are used across these genome methods: silencing random genes, unsilencing random genes, and nooping random genes. Silencing a gene activates its "silent" epigenetic marker, making that instruction not appear the resulting Push program, as well as not ending any open parenthesized code blocks. When picking a random gene to silence, we never select a gene that is already silent.

In some methods we silence some genes while unsilencing others, which makes it possible to backtrack by activating genes that were previously turned off. We hope that the backtracking enabled by unsilencing will allow simplification to escape some local program size minima that it might otherwise be impossible to escape, resulting in more reliable simplification to smaller programs. Similar to silencing, we never select an unsilenced gene to unsilence. Note that we silence genes instead of removing them entirely because it makes it possible to backtrack progress by unsilencing genes.

Finally, some methods "noop" genes by replacing their instructions with instructions that have no effect when executed. Some Push programs require some instruction to be present in a location, but it does not matter what the instruction is as long as it doesn't manipulate certain stacks. We can replace these instructions with noops, potentially making the program more general without actually making it smaller. Genes that are already nooped cannot be selected by a noop step of simplification.

\begin{table}[t]
	\centering
	\caption{Genome simplification methods. Each variant lists the possible single simplification steps it can take during simplification, and the probability of taking each step.}
	\label{table:genome-methods}
	\begin{tabu} to \textwidth {l l r}
		\toprule
		\textbf{Variant} & \textbf{Step} & \textbf{Prob.}  \\
		\midrule
		\textbf{Genome} & Silence 1 (gene) & 0.50 \\
		       & Silence 2 & 0.30 \\
		       & Silence 3 & 0.10 \\
		       & Silence 4 & 0.10 \\
		\midrule
		\textbf{Genome-Backtracking}
		       & Silence 1 & 0.40 \\
		       & Silence 2 & 0.25 \\
		       & Silence 3 & 0.10 \\
		       & Silence 4 & 0.05 \\
		       & Silence 1, Unsilence 1 & 0.05 \\
		       & Silence 2, Unsilence 1 & 0.10 \\
		       & Silence 3, Unsilence 1 & 0.05 \\
		\midrule
		\textbf{Genome-Noop}
		       & Silence 1 & 0.40 \\
		       & Silence 2 & 0.25 \\
		       & Silence 3 & 0.10 \\
		       & Silence 4 & 0.05 \\
		       & Noop 1 & 0.10 \\
		       & Noop 2 & 0.10 \\
		\midrule
		\textbf{Genome-Backtracking-}
		       & Silence 1 & 0.30 \\
		  \textbf{Noop} & Silence 2 & 0.20 \\
		       & Silence 3 & 0.10 \\
		       & Silence 1, Unsilence 1 & 0.05 \\
		       & Silence 2, Unsilence 1 & 0.10 \\
		       & Silence 3, Unsilence 1 & 0.05 \\
		       & Noop 1 & 0.10 \\
		       & Noop 2 & 0.10 \\
		\bottomrule
	\end{tabu}
\end{table}

The four genome methods of simplification are listed in Table \ref{table:genome-methods}. Each variant lists the types of steps that can be taken and the probability of each step being chosen. The most basic of the methods, \textbf{Genome} only allows for the silencing of 1 to 4 random genes in a single step. \textbf{Genome-Backtracking} and \textbf{Genome-Backtracking-Noop} occasionally unsilence a silent gene while silencing another, allowing the process to not follow a strict hill-climbing scheme.  \textbf{Genome-Noop} and Genome-Backtracking-Noop use nooping to turn intructions into noops without removing them entirely.



\section{Experimental setup}
\label{sec:setup}

In our experiments, we wanted to answer two primary questions. First, does automatic simplification effectively improve the generalization of evolved programs that pass every training case? Second, which of the simplication methods described in the previous section produces the smallest programs, which generalizes the best, and is there correlation between getting smaller and generalizing better?

To examine these questions, we conducted simplification experiments using evolved solution programs from a suite of general program synthesis benchmark problems \cite{Helmuth:2015:GECCO}. This benchmark suite is composed of 29 general programming problems taken from introductory computer science course homework assignments. These problems require solution programs to utilize a wide range of programming constructs, such as multiple data types, control flow structures, and multiple inputs/outputs. For these problems, we are primarily interested in whether we can find a program that passes every training case and unseen test case, since programs that do not achieve such perfection do not represent true solutions to the problems \cite{Helmuth:2015:GECCO}. We will call a program that passes all of the training set a ``solution'', and a program that additionally passes all of the unseen test set a ``generalizing solution.''

After the original publication of these benchmark problems \cite{Helmuth:2015:GECCO}, they have seen use in a range of studies using PushGP (eg. \cite{Helmuth:2016:GECCO, McPhee:2016:GPTP, Helmuth:2015:GPTP, Helmuth:2015:dissertation}). Additionally, they were used in work considering a general program synthesis grammar for grammar guided genetic programming \cite{Forstenlechner:2017:eurogp}.

We took our evolved solution programs from the original set of benchmarking runs using the benchmark suite \cite{Helmuth:2015:GECCO}. In those original runs, each problem was attempted 100 times per selection method; in this work, we only use the runs that used lexicase selection. In these runs, PushGP created at least one generalizing solution to 22 of the problems. In addition, we include one problem (Super Anagrams) for which 14 programs were evolved that passed the training set but did not pass the unseen test set. We also include the Checksum problem, which was not solved in the runs for the original publication, but was subsequently solved with some additions to the training set.

For each of our five simplification methods, we conducted 100 separate simplification trials of each solution program, recording the size and generalization of each simplified program. Some of the unsimplified programs also entirely passed the unseen test set (i.e. generalized), and some did not (i.e. did not generalize). When running GP on real-world problems, one will not know whether an evolved solution program generalizes or not before using simplification. Thus we are interested in not only what happens to evolved programs that do not generalize, but also to those that already generalize. For example, it would not be beneficial to improve generalization of some programs while reducing generalization of many more programs.

For each simplification trial, we simplified the solution program using 10,000 steps of the simplification algorithm (see Algorithm~\ref{alg:simplification}). While this number of steps is likely more than necessary to effectively simplify most programs, the simplification process is not prohibitively expensive. Note that while every simplification step requires evaluating the new program, making 10,000 simplification steps moderately expensive, this is the equivalent number of evaluations to 10 generations of GP with a population of 1000. Since we are advocating for simplification at the end of a GP run, it only needs to happen once, and seems like a reasonable amount of required computation.

The benchmark suite we used prescribes using randomly generated training and test cases for most of the input and output data. Since automatic simplification requires use of training data, we use the training and test sets that were used for the original run for every simplification of the solution program from that run.



\section{Results}
\label{sec:results}

We first consider the question of how much effect simplification has on the size of each solution program. Figure~\ref{fig:bar:size} presents the average program size of the unsimplified solution programs on each problem as a horizontal black line. It then gives the average simplified size of the simplified solution programs across 100 trials per solution for each of our five simplification methods. It is immediately clear that simplification has a large impact on the size of these programs, with most shrinking to half their original size or smaller.


\begin{figure*}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\textwidth]{Problem_by_Size_bar} % \linewidth
\caption{For each method, the average program size of simplified programs.}
\label{fig:bar:size}
\end{figure*}

To examine aggregate performance of each simplification method, we calculate the average rank for size for each method across the 24 problems, with 1 having smallest average program size and 6 having largest. Note that we include the unsimplified programs, which is why we have 6 possible ranks. These are in Table~\ref{table:size-ranks}. \hl{Explain which has the best size numbers and Why.}

\begin{table}[ht]
	\centering
	\caption{The average rank in size for each simplification variant, where lower rank means smaller programs. This also includes the pre-simplification programs. Note the different order of methods from Table~\ref{table:generalization-ranks}.}
	\label{table:size-ranks}
	\begin{tabu} to \textwidth {l r}
		\toprule
		\textbf{Variant} & \textbf{Average Rank} \\
		\midrule
		\textbf{Program} & 1.87 \\
		\textbf{Genome-Backtracking-Noop} & 2.13 \\
		\textbf{Genome-Backtracking} & 2.79 \\
		\textbf{Genome-Noop} & 3.60 \\
		\textbf{Genome} & 4.60 \\
		\textbf{Pre-Simplification} & 6.00 \\
		\bottomrule
	\end{tabu}
\end{table}


\hl{Lexicase achieves the lowest average rank, as it has the most or tied for the most successes on every problem except for one. The Friedman test on this data gives us a \textit{p}-value $< 0.001$, indicating that at least one method} performs significantly differently from the others. A post-hoc Wilcoxon-Nemenyi-McDonald-Thompson test \cite{hollander1999nonparametric} can give the significance in the differences in ranking between each pair of methods at the $0.05$ significance level. Here, we see that:
\begin{itemize}
\item Program outranks Genome-Noop, Genome, and Pre-Simplification
\item Genome-Backtracking-Noop outranks Genome and Pre-Simplification
\item Genome-Backtracking outranks Genome and Pre-Simplification
\item Genome-Noop outranks Pre-Simplification
\end{itemize}
These results show that all simplification methods besides Genome have significantly better rank than if we do not use simplification. \hl{What else to say here?}

%Program - none                              3.685940e-14
%GenomeBacktrackingNoop - none               6.733392e-12
%GenomeBacktracking - none                   1.343098e-08
%Program - Genome                            4.483262e-06
%GenomeBacktrackingNoop - Genome             4.216509e-05
%GenomeNoop - none                           9.320840e-05
%GenomeBacktracking - Genome                 8.811779e-03
%Program - GenomeNoop                        1.506233e-02
%---------------------------------------------------------
%  Cutoff for p-value of 0.05 or less (above)
%---------------------------------------------------------
%GenomeNoop - GenomeBacktrackingNoop         6.179964e-02
%Genome - none                               9.284012e-02
%GenomeNoop - Genome                         4.175994e-01
%Program - GenomeBacktracking                5.190458e-01
%GenomeNoop - GenomeBacktracking             6.489706e-01
%GenomeBacktrackingNoop - GenomeBacktracking 8.119145e-01
%Program - GenomeBacktrackingNoop            9.971909e-01

Next, we consider the effect of simplification on the generalization of programs to unseen test data. In Figure~\ref{fig:bar:percent_generalization}, we plot the percent of programs that generalize for each simplification method, as well as a horizontal bar representing the percent of unsimplified programs that generalize.

\begin{figure*}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\textwidth]{Problem_by_PcntGen_bar} % \linewidth
\caption{For each method, the percent of simplified programs that generalize to unseen data. \hl{Question: In this figure, how much of the time are the simplified programs significantly better at generalization than the unsimplified programs? How would we measure this? Do we need to?}}
\label{fig:bar:percent_generalization}
\end{figure*}

Discuss: simplification either helps generalization or has no effect on every problem besides Double Letters. Double Letters has one of the smaller sample sizes, with only 6 solution programs as starting points. All generalize before simplification.  5 of the 6 generalize most of the time after simplification, but one program has many simplifications that don't generalize. Mostly seems like an outlier here.

To see the effect of each simplification method on generalization, we found the average rank of each method across all problems. Here, rank 1 signifies the best (highest) generalization, and rank 6 represents the worst generalization. These average ranks are presented in Table~\ref{table:generalization-ranks}.

\begin{table}[ht]
	\centering
	\caption{The average rank in generalization percent for each simplification variant, where lower rank means better generalization. This also includes the pre-simplification programs.}
	\label{table:generalization-ranks}
	\begin{tabu} to \textwidth {l r}
		\toprule
		\textbf{Variant} & \textbf{Average Rank} \\
		\midrule
		\textbf{Genome-Backtracking-Noop} & 2.73 \\
		\textbf{Genome-Backtracking} & 3.02 \\
		\textbf{Genome-Noop} & 3.29 \\
		\textbf{Genome} & 3.33 \\
		\textbf{Program} & 3.67 \\
		\textbf{Pre-Simplification} & 4.96 \\
		\bottomrule
	\end{tabu}
\end{table}


%GenomeBacktrackingNoop - none               1.532704e-06
%GenomeBacktracking - none                   8.164085e-05
%GenomeNoop - none                           1.186298e-03
%Genome - none                               1.892900e-03
%Program - none                              2.829973e-02
%%---------------------------------------------------------
%%  Cutoff for p-value of 0.05 or less (above)
%%---------------------------------------------------------
%Program - GenomeBacktrackingNoop            2.338544e-01
%Program - GenomeBacktracking                6.504160e-01
%GenomeNoop - GenomeBacktrackingNoop         7.712306e-01
%GenomeBacktrackingNoop - Genome             7.129661e-01
%GenomeBacktracking - Genome                 9.774709e-01
%GenomeNoop - Genome                         9.999987e-01
%Program - Genome                            9.701077e-01
%GenomeBacktrackingNoop - GenomeBacktracking 9.834383e-01
%GenomeNoop - GenomeBacktracking             9.881573e-01
%Program - GenomeNoop                        9.505945e-01


\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\linewidth]{Size_density_by_generalized_A_no_facet_count}
\caption{Size counts of simplified programs where pre-simplified program did generalize.}
\label{fig:count:pre-simp-gen-true}
\end{figure}

\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\linewidth]{Size_density_by_generalized_B_no_facet_count}
\caption{Size counts of simplified programs where pre-simplified program did \underline{not} generalize.}
\label{fig:count:pre-simp-gen-true}
\end{figure}

\hl{Results!}

%\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
%\centering
%\includegraphics[width=\linewidth]{Size_density_by_generalized_count}
%\caption{Size counts of simplified programs. Includes all simplified programs, from both ones that did and did not generalize pre-simplification.}
%\label{fig:count:gen}
%\end{figure}

%These look at the same graphs, but faceted.
%
%\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
%\centering
%\includegraphics[width=\linewidth]{Size_density_by_generalized_A_count}
%\caption{Size counts of simplified programs where pre-simplified program did generalize.}
%\label{fig:count:pre-simp-gen-true}
%\end{figure}
%
%\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
%\centering
%\includegraphics[width=\linewidth]{Size_density_by_generalized_B_count}
%\caption{Size counts of simplified programs where pre-simplified program did \underline{not} generalize.}
%\label{fig:count:pre-simp-gen-true}
%\end{figure}

MORE FIGURES!

\begin{figure*}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\textwidth]{Nic_Plot_2} % \linewidth
\caption{[This figure is representative of 2 or 3 other similar figures that might fit these needs.] First row: pre- and post-simplification both didn't generalize. Second row: didn't generalize pre-simplification, then generalized post-simplification. Etc. The lines are linear fit lines. You can see that programs that generalized post-simplification have smaller slope on the trend lines!}
\label{fig:nic-plot}
\end{figure*}



\section{Discussion}
\label{sec:discussion}

\todo[inline]{Talk about what the results mean}

Here are some potentially interesting questions to explore with this data:

    Which simplification method tends to result in the smallest programs on average? Is there any significant differences here?

    Is getting smaller during simplification important for generalization? In other words, do smaller programs post-simplification tend to generalize better than larger ones?

        Note: we could ask this question of all the simplified programs, but we could also concentrate only on runs that had some generalizing programs and some non-generalizing programs, which might give us a more detailed view. It would probably be worthwhile to look into both of these. I'm not sure exactly how we would investigate this. @mcphee made some cool graphs last year that start answering this question. I've included one at the bottom of this post that was made with limited data; a similar graph for the full data might be a good place to start.

    It might be worthwhile to break this big plot into smaller plots, grouping together similar plots. For example, we might group plots based on how often unsimplified programs generalize -- groups of something like 0-50\%, 50-95\%, and 95-100\% generalization. Or, we could do it some other way. But, this could definitely help with spotting differences based on problem.

    Would a multi-restart simplification get us to smaller programs more often than the other methods we've tried? This could potentially avoid unlucky local minima. The backtracking methods are also trying to avoid unlucky local minima, so this would be an interesting comparison. It seems like we could answer this question with the data we have.

\subsection{Nic's Thoughts on Unused Push Instructions}
Anyway, I wouldn't recommend using the word "bloat" because I also think that is generally associated with a tendency to get bigger and bigger and bigger in ways that aren't connected to improvements in fitness, and that doesn't seem to be a Clojush thing. (I'm personally not super clear on why that's not a thing in Clojush, etc., etc., but that's not today's problem.)

Even more complicated is the fact that in Push programs we often find various categories of "unused" code. There are instructions that do nothing because their arguments aren't on the stack, or sometimes aren't on the stack. There are instructions that do something, but no one cares (they push a boolean that's never used). There are instructions that are there just to add content to a stack, but who's particular action doesn't matter. Etc., etc. These are not things "standard tree-based GP" people run into very often, and might not be obvious to them.

Lastly, it can be really hard to tell what's being used and how in complex programs, and how that relates to the simplified versions. It's easy to implicitly assume that the simplified program is "the thing", and that everything that got removed didn't really matter in the original execution. (This is often an assumption in tree-based bloat discussions.) I'm pretty sure this is often very not true, however, in Push, and that's important to the generalization. It seems fairly common, for example, for Push programs (especially with the "standard" instruction set) to have a lot of unnecessary looping that frequently gets removed or reduced in simplification. Often that looping is actually executed in the original program, so it's part of how the training answers are formed. I have a suspicion that part of why simplification often improves generalization is that it removes these unnecessary loops that are sometimes fragile and do weird things on certain inputs.



\section{Related Work}
\label{sec:related}

\todo[inline]{Tom: I'm fine moving this section earlier if others prefer.}

\todo[inline]{Below are things I've (Tom) come across in the past year potentially realted to this work. We'll have to go through and see what's worth citing.}

\subsection{Papers about automatic simplification}

\begin{itemize}

\item
Genprog minimization after run (see 7/24/16)

\item
Field Guide to Genetic Programming: p 64 top: Bahnzaf paper might be precursor to automatic simplification

\item
This paper in neural networks might be related: GECCO 2016 - Identifying Core Functional Networks and Functional Modules within Artificial Neural Networks via Subsets Regression

\item
Differentiate between our work and algebraic simplification (as used in semantic GP), since this CAN change the semantics on inputs not in the training set, where algebraic methods cannot. Also, algebraic methods wouldn't work on general programs that we're evolving

\item
Algebraic Simplification of GP Programs During Evolution (I think this is an actual paper title)

\item
Investigation of simplification threshold and noise level of input data in numerical simplification of genetic programs \cite{Kinzett:2010:cec}

\item
Smaller networks in neural nets: https://push-language.hampshire.edu/t/gecco-2017-simplification-for-generalization/660/21?u=thelmuth

\end{itemize}

\subsection{Papers about generalization and overfitting in GP}

\begin{itemize}
\item
GECCO 2011 - Variance based Selection to Improve Test Set Performance in Genetic Programming

\item
Controlling overfitting in symbolic regression based on a bias/variance error decomposition


\end{itemize}

\subsection{Other related papers}

\begin{itemize}
\item
Tree-structured differencing: R. Al-Ekram, A. Adma, and O. Baysal. diffX: an algorithm to detect changes in multi-version XML documents. In Conference of the Centre for Advanced Studies on Collaborative research, pages 1–11. IBM Press, 2005.

\item
Delta debugging: A. Zeller. Yesterday, my program worked. Today, it does not. Why? In Foundations of Software Engineering, pages 253–267, 1999.

\end{itemize}

\section{Conclusions and future work}
\label{sec:conclusions}

\todo[inline]{Maybe this could/should be two sections, but I bet we won't have room.}

\begin{acks}
  
  \todo[inline]{Put acknowledgements here, including grants.}

\end{acks}
