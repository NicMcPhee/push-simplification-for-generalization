\section{Introduction}
\label{sec:intro}

\todo[inline]{Motivation and overview.}

Things we need to do somewhere in this paper:
\begin{itemize}
	\item Convince everyone that this is awesome. It would be particularly cool if we can somehow make the case that these ideas are relevant even if people aren't doing stack based work. Can we do that, though? Most (i.e., tree-based)
	GP systems don't allow for things like silencing of nodes or replacing nodes
	with NOOPs because it typically wouldn't be clear how to interpret such a tree.
	\item Cover relevant work. This might require some digging?
	\item Cover relevant background. This is going to (at least) require explaining PLUSH genomes, the conversion from PLUSH to PUSH, the evaluation of PUSH programs (so we can understand what NOOPs do), and silent genes.
	\item Describe the 5 simplification methods
	\item Experimental setup
	\item Results
	\item Discussion
	\item Future work
	\item Conclude
\end{itemize}

Smaller solutions have other benefits as well: understandability, faster runtimes, etc. But, we won't concentrate on those in this paper.

Automatic simplification was introduced to Push as a tool for making evolved programs easier to understand \cite{Robinson:2001:GPtieus, spector:2002:GPEM}. While it can be applied to any program, it has typically been used post-run to make solution programs easier to understand without changing their behavior \cite{Spector:2014:GECCOcomp}. It has also been used as a bloat-control genetic operator during genetic programming runs, with mixed success \cite{Zhan:2014:GECCOcomp}. \todo[inline]{Does this paragraph belong here in the intro or in the Simplification section?}

\section{PUSH, PLUSH, etc.}
\label{sec:push}

\todo[inline]{Explain necessary aspects of PUSH, PLUSH, and the like. Include talk of silencing genes, and how a silenced gene does not affect the resulting program, including instructions and closing parentheses.}

\todo[inline]{We should briefly define what the ``size'' of a push program is, since we discuss size a lot later.}

\section{Automatic Simplification in Push}
\label{sec:simplification}



Automatic simplification is a simple hill-climbing algorithm that tries to make an individual's program smaller without changing the program's behavior on the training cases. At each step, we first make small changes to the program to make it smaller. We then check whether the smaller program produces the same error vector as the original program. If so, we continue with the new program; otherwise, we reverts to the previous program. This process repeats for a set number of simplification steps, and then returns the resulting simplified program. The algorithm is presented in detail in Algorithm \ref{alg:simplification}.

\begin{algorithm}[ht]
\caption{Automatic Simplification}
\label{alg:simplification}

\SetKwInOut{Input}{Input}
\SetKwData{ErrorVector}{errorVector}
\SetKwData{NewErrorVector}{newErrorVector}
\SetKwData{Ind}{ind}
\SetKwData{NewInd}{newInd}
\SetKwData{Steps}{steps}
\SetKwData{S}{s}
\SetKwFunction{ComputeErrors}{computeErrorVector}
\SetKwFunction{TakeStep}{takeSimplificationStep}

\Input{individual \Ind, number of simplification \Steps, method for simplification step \TakeStep}
\BlankLine

\ErrorVector $\leftarrow$ \ComputeErrors{\Ind} \;
\For{\S $\leftarrow 0$ \KwTo \Steps}{

	\NewInd $\leftarrow$ \TakeStep{\Ind} \;
	\NewErrorVector $\leftarrow$ \ComputeErrors{\NewInd} \;
	\If{\NewErrorVector $=$ \ErrorVector}{
		\Ind $\leftarrow$ \NewInd
	}
}
\Return \Ind
\end{algorithm}

In this paper we explore five different methods of automatic simplification. Each method follows the same general algorithm; they only differ in the particular steps taken to make programs smaller. %The five methods are listed in Table ???, along with a description of what they do at each simplification step.
Below is a description of each method:

\textbf{Program} simplification, which has been used in Push since its inception \cite{Robinson:2001:GPtieus}, acts on an individual's Push program, as opposed to its linear Plush genome. At each simplification step, it has an 80\% chance of removing one or two random "code points" from the program, where a \textit{code point} is an instruction, a constant, or a parenthesized block of code. The other 20\% of the time it randomly removes one set of parentheses from a code block in the program, flattening the code inside the parentheses into the level that the code block occupied.

The other four simplification methods act on an individual's linear Plush genome prior to its translation into a Push program. There are three basic steps that are used across these genome methods: silencing random genes, unsilencing random genes, and nooping random genes. Silencing a gene activates its "silent" epigenetic marker, making that instruction not appear the resulting Push program, as well as not ending any open parenthesized code blocks. When picking a random gene to silence, we never select a gene that is already silent.

In some methods we silence some genes while unsilencing others, which makes it possible to backtrack by activating genes that were previously turned off. We hope that the backtracking enabled by unsilencing will allow simplification to escape some local program size minima that it might otherwise be impossible to escape, resulting in more reliable simplification to smaller programs. Similar to silencing, we never select an unsilenced gene to unsilence. Note that we silence genes instead of removing them entirely because it makes it possible to backtrack progress by unsilencing genes.

Finally, some methods "noop" genes by replacing their instructions with instructions that have no effect when executed. Some Push programs require some instruction to be present in a location, but it does not matter what the instruction is as long as it doesn't manipulate certain stacks. We can replace these instructions with noops, potentially making the program more general without actually making it smaller. Genes that are already nooped cannot be selected by a noop step of simplification.

\begin{table}[t]
	\centering
	\caption{Genome simplification methods. Each variant lists the possible single simplification steps it can take during simplification, and the probability of taking each step.}
	\label{table:genome-methods}
	\begin{tabu} to \textwidth {l l r}
		\toprule
		\textbf{Variant} & \textbf{Step} & \textbf{Prob.}  \\
		\midrule
		\textbf{Genome} & Silence 1 (gene) & 0.50 \\
		       & Silence 2 & 0.30 \\
		       & Silence 3 & 0.10 \\
		       & Silence 4 & 0.10 \\
		\midrule
		\textbf{Genome-Backtracking}
		       & Silence 1 & 0.40 \\
		       & Silence 2 & 0.25 \\
		       & Silence 3 & 0.10 \\
		       & Silence 4 & 0.05 \\
		       & Silence 1, Unsilence 1 & 0.05 \\
		       & Silence 2, Unsilence 1 & 0.10 \\
		       & Silence 3, Unsilence 1 & 0.05 \\
		\midrule
		\textbf{Genome-Noop}
		       & Silence 1 & 0.40 \\
		       & Silence 2 & 0.25 \\
		       & Silence 3 & 0.10 \\
		       & Silence 4 & 0.05 \\
		       & Noop 1 & 0.10 \\
		       & Noop 2 & 0.10 \\
		\midrule
		\textbf{Genome-Backtracking-}
		       & Silence 1 & 0.30 \\
		  \textbf{Noop} & Silence 2 & 0.20 \\
		       & Silence 3 & 0.10 \\
		       & Silence 1, Unsilence 1 & 0.05 \\
		       & Silence 2, Unsilence 1 & 0.10 \\
		       & Silence 3, Unsilence 1 & 0.05 \\
		       & Noop 1 & 0.10 \\
		       & Noop 2 & 0.10 \\
		\bottomrule
	\end{tabu}
\end{table}

The four genome methods of simplification are listed in Table \ref{table:genome-methods}. Each variant lists the types of steps that can be taken and the probability of each step being chosen. The most basic of the methods, \textbf{Genome} only allows for the silencing of 1 to 4 random genes in a single step. \textbf{Genome-Backtracking} and \textbf{Genome-Backtracking-Noop} occasionally unsilence a silent gene while silencing another, allowing the process to not follow a strict hill-climbing scheme.  \textbf{Genome-Noop} and Genome-Backtracking-Noop use nooping to turn intructions into noops without removing them entirely.



\section{Experimental setup}
\label{sec:setup}

In our experiments, we wanted to answer two primary questions. First, does automatic simplification effectively improve the generalization of evolved programs that pass every training case? Second, which of the simplication methods described in the previous section produces the smallest programs, which generalizes the best, and is there correlation between getting smaller and generalizing better?

To examine these questions, we conducted simplification experiments using evolved solution programs from a suite of general program synthesis benchmark problems \cite{Helmuth:2015:GECCO}. This benchmark suite is composed of 29 general programming problems taken from introductory computer science course homework assignments. These problems require solution programs to utilize a wide range of programming constructs, such as multiple data types, control flow structures, and multiple inputs/outputs. For these problems, we are primarily interested in whether we can find a program that passes every training case and unseen test case, since programs that do not achieve such perfection do not represent true solutions to the problems \cite{Helmuth:2015:GECCO}. We will call a program that passes all of the training set a ``solution'', and a program that additionally passes all of the unseen test set a ``generalizing solution.''

After the original publication of these benchmark problems \cite{Helmuth:2015:GECCO}, they have seen use in a range of studies using PushGP (eg. \cite{Helmuth:2016:GECCO, McPhee:2016:GPTP, Helmuth:2015:GPTP, Helmuth:2015:dissertation}). Additionally, they were used in work considering a general program synthesis grammar for grammar guided genetic programming \cite{Forstenlechner:2017:eurogp}.

We took our evolved solution programs from the original set of benchmarking runs using the benchmark suite \cite{Helmuth:2015:GECCO}. In those original runs, each problem was attempted 100 times per selection method; in this work, we only use the runs that used lexicase selection. In these runs, PushGP created at least one generalizing solution to 22 of the problems. In addition, we include one problem (Super Anagrams) for which 14 programs were evolved that passed the training set but did not pass the unseen test set. We also include the Checksum problem, which was not solved in the runs for the original publication, but was subsequently solved with some additions to the training set.

In earlier work showing how post-run simplification can reduce the size of evolved solution programs, it was shown that simplifying the same program many times can result in a range of resulting program sizes \cite{Spector:2014:GECCOcomp}. To acount for this variance in the simplified programs, we performed multiple simplifications of each solution program. For each of our five simplification methods, we conducted 100 separate simplification trials of each solution program, recording the size and generalization of each simplified program. Some of the unsimplified programs also entirely passed the unseen test set (i.e. generalized), and some did not (i.e. did not generalize). When running GP on real-world problems, one will not know whether an evolved solution program generalizes or not before using simplification. Thus we are interested in not only what happens to evolved programs that do not generalize, but also to those that already generalize. For example, it would not be beneficial to improve generalization of some programs while reducing generalization of many more programs.

For each simplification trial, we simplified the solution program using 10,000 steps of the simplification algorithm (see Algorithm~\ref{alg:simplification}). While this number of steps is likely more than necessary to effectively simplify most programs, the simplification process is not prohibitively expensive. Note that while every simplification step requires evaluating the new program, making 10,000 simplification steps moderately expensive, this is the equivalent number of evaluations to 10 generations of GP with a population of 1000. Since we are advocating for simplification at the end of a GP run, it only needs to happen once, and seems like a reasonable amount of required computation.

The benchmark suite we used prescribes using randomly generated training and test cases for most of the input and output data. Since automatic simplification requires use of training data, we use the training and test sets that were used for the original run for every simplification of the solution program from that run.



\section{Results}
\label{sec:results}

\begin{figure*}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\textwidth]{Problem_by_Size_bar} % \linewidth
\caption{For each problem, we plot the average program size of programs produced by different simplification methods, as well as the average program sizes of the unchanged programs as black horizontal bars. Remember that each simplification method does repeated trials using the same set of unchanged programs, and therefore has the same opportunities for simplification as the other methods.}
\label{fig:bar:size}
\end{figure*}

We first consider the question of how much effect simplification has on the size of each solution program. Figure~\ref{fig:bar:size} presents the average program size of the unsimplified solution programs on each problem as a horizontal black line. It then gives the average simplified size of the simplified solution programs across 100 trials per solution for each of our five simplification methods. It is immediately clear that simplification has a large impact on the size of these programs, with most shrinking to half their original size or smaller.

Note that all simplification methods produce very similar reductions in program size, yet all are smaller than the unchanged programs they come from.

To examine aggregate performance of each simplification method, we calculate the average rank for size for each method across the 24 problems, with 1 having smallest average program size and 6 having largest. Note that we include the unsimplified programs, which is why we have 6 possible ranks. These are in Table~\ref{table:size-ranks}. \hl{Explain which method has the best size numbers and Why.}

\begin{table}[ht]
	\centering
	\caption{The average rank in size for each simplification method across the data in Figure~\ref{fig:bar:size}, where lower rank means smaller programs. ``Unchanged'' is the rank of the evolved programs without any simplification. Methods are sorted by average rank.}
	\label{table:size-ranks}
	\begin{tabu} to \textwidth {l r}
		\toprule
		\textbf{Method} & \textbf{Average Rank} \\
		\midrule
		\textbf{Program} & 1.87 \\
		\textbf{Genome-Backtracking-Noop} & 2.13 \\
		\textbf{Genome-Backtracking} & 2.79 \\
		\textbf{Genome-Noop} & 3.60 \\
		\textbf{Genome} & 4.60 \\
		\textbf{Unchanged} & 6.00 \\
		\bottomrule
	\end{tabu}
\end{table}


\hl{Lexicase achieves the lowest average rank, as it has the most or tied for the most successes on every problem except for one. The Friedman test on this data gives us a \textit{p}-value $< 0.001$, indicating that at least one method} performs significantly differently from the others. A post-hoc Wilcoxon-Nemenyi-McDonald-Thompson test \cite{hollander1999nonparametric} can give the significance in the differences in ranking between each pair of methods at the $0.05$ significance level. Here, we see that:
\begin{itemize}
\item Program outranks Genome-Noop, Genome, and Pre-Simplification
\item Genome-Backtracking-Noop outranks Genome and Pre-Simplification
\item Genome-Backtracking outranks Genome and Pre-Simplification
\item Genome-Noop outranks Pre-Simplification
\end{itemize}
These results show that all simplification methods besides Genome have significantly better rank than if we do not use simplification. \hl{What else to say here?}

%Program - none                              3.685940e-14
%GenomeBacktrackingNoop - none               6.733392e-12
%GenomeBacktracking - none                   1.343098e-08
%Program - Genome                            4.483262e-06
%GenomeBacktrackingNoop - Genome             4.216509e-05
%GenomeNoop - none                           9.320840e-05
%GenomeBacktracking - Genome                 8.811779e-03
%Program - GenomeNoop                        1.506233e-02
%---------------------------------------------------------
%  Cutoff for p-value of 0.05 or less (above)
%---------------------------------------------------------
%GenomeNoop - GenomeBacktrackingNoop         6.179964e-02
%Genome - none                               9.284012e-02
%GenomeNoop - Genome                         4.175994e-01
%Program - GenomeBacktracking                5.190458e-01
%GenomeNoop - GenomeBacktracking             6.489706e-01
%GenomeBacktrackingNoop - GenomeBacktracking 8.119145e-01
%Program - GenomeBacktrackingNoop            9.971909e-01

Next, we consider the effect of simplification on the generalization of programs to unseen test data. In Figure~\ref{fig:bar:percent_generalization}, we plot the percent of programs that generalize for each simplification method, as well as a horizontal bar representing the percent of unsimplified programs that generalize.

\begin{figure*}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\textwidth]{Problem_by_PcntGen_bar} % \linewidth
\caption{For each problem, we plot the percent of simplified programs that generalize to unseen data, as well as the percent of unchanged programs that generalize as black horizontal bars.
 \hl{Question: In this figure, how much of the time are the simplified programs significantly better at generalization than the unsimplified programs? How would we measure this? Do we need to?}}
\label{fig:bar:percent_generalization}
\end{figure*}

Discuss: simplification either helps generalization or has no effect on every problem besides Double Letters. Double Letters has one of the smaller sample sizes, with only 6 solution programs as starting points. All generalize before simplification.  5 of the 6 generalize most of the time after simplification, but one program has many simplifications that don't generalize. Mostly seems like an outlier here.

Discuss: note correlation between problems that had largest program sizes after simplification are those with worst generalization.

To see the effect of each simplification method on generalization, we found the average rank of each method across all problems. Here, rank 1 signifies the best (highest) generalization, and rank 6 represents the worst generalization. These average ranks are presented in Table~\ref{table:generalization-ranks}.

\begin{table}[ht]
	\centering
	\caption{The average rank in generalization for each simplification method across the problems in Figure~\ref{fig:bar:percent_generalization}, where lower rank means better generalization. ``Unchanged'' is the rank of the evolved programs without any simplification. Methods are sorted by average rank.}
	\label{table:generalization-ranks}
	\begin{tabu} to \textwidth {l r}
		\toprule
		\textbf{Method} & \textbf{Average Rank} \\
		\midrule
		\textbf{Genome-Backtracking-Noop} & 2.73 \\
		\textbf{Genome-Backtracking} & 3.02 \\
		\textbf{Genome-Noop} & 3.29 \\
		\textbf{Genome} & 3.33 \\
		\textbf{Program} & 3.67 \\
		\textbf{Unchanged} & 4.96 \\
		\bottomrule
	\end{tabu}
\end{table}

%GenomeBacktrackingNoop - none               1.532704e-06
%GenomeBacktracking - none                   8.164085e-05
%GenomeNoop - none                           1.186298e-03
%Genome - none                               1.892900e-03
%Program - none                              2.829973e-02
%%---------------------------------------------------------
%%  Cutoff for p-value of 0.05 or less (above)
%%---------------------------------------------------------
%Program - GenomeBacktrackingNoop            2.338544e-01
%Program - GenomeBacktracking                6.504160e-01
%GenomeNoop - GenomeBacktrackingNoop         7.712306e-01
%GenomeBacktrackingNoop - Genome             7.129661e-01
%GenomeBacktracking - Genome                 9.774709e-01
%GenomeNoop - Genome                         9.999987e-01
%Program - Genome                            9.701077e-01
%GenomeBacktrackingNoop - GenomeBacktracking 9.834383e-01
%GenomeNoop - GenomeBacktracking             9.881573e-01
%Program - GenomeNoop                        9.505945e-01





\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\linewidth]{Size_density_by_generalized_B_no_facet_count}
\caption{Size counts of simplified programs where pre-simplified program did \underline{not} generalize.}
\label{fig:count:pre-simp-gen-false}
\end{figure}

\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\linewidth]{Size_density_by_generalized_A_no_facet_count}
\caption{Size counts of simplified programs where pre-simplified program did generalize.}
\label{fig:count:pre-simp-gen-true}
\end{figure}

Figures~\ref{fig:count:pre-simp-gen-false} and \ref{fig:count:pre-simp-gen-true} each plot the counts of the sizes of the simplified programs for different sets of programs, aggregated across all problems. Figure~\ref{fig:count:pre-simp-gen-false} only includes those simplifications that started with unchanged programs that did not generalize. This figure clearly shows that when the simplified program was relatively small (approximately of size less than 25), it was much more likely to change from ungeneralizing to generalizing (shown by the blue line). On the other hand, programs that remained larger after simplification were more likely to not have their generalization affected by simplification (shown by the red line). Thus, most of the improvements in generalization that we described previously come from simplified programs that achieve small sizes. This clearly shows that there is a correlation, if not causation, between post-simplification size and the probability of generalization.

Figure~\ref{fig:count:pre-simp-gen-true} gives the same counts of sizes of the simplified programs, but only for those unchanged programs that did generalize. First, note that the scale of the $y$-axis is over 5 times as large as for Figure~\ref{fig:count:pre-simp-gen-false}; this tells us that many more of the unchanged programs did generalize (\hl{is this true???}). Here, we are interested in whether many of the unchanged programs that did generalize were broken by simplification to the point of not generalizing. We see that very few such instances occured compared to those that remained generalizing after simplification. Thus while simplification often makes a program go from ungeneralizing to generalizing, it does not often take a generalizing program and make it ungeneralizing.

The correlation between post-simplification size and generalization can also be seen in Figure~\ref{fig:nic-plot}. Here, each point represents one simplified program; which plot it belongs to depends on whether it generalized before or after simplification. The $x$-axis gives pre-simplification size of the program, and the $y$-axis gives simplified program size; thus, every point must lie under the $y = x$ line. Points close to the $y = x$ line did not reduce much during simplification; those further below the $y = x$ line correspond to those that shrank more.

\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
\centering
\includegraphics[width=\linewidth]{Nic_Plot_2} % \linewidth
\caption{[This figure is representative of 2 or 3 other similar figures that might fit these needs.] First row: pre- and post-simplification both didn't generalize. Second row: didn't generalize pre-simplification, then generalized post-simplification. Etc. The lines are linear fit lines. You can see that programs that generalized post-simplification have smaller slope on the trend lines!}
\label{fig:nic-plot}
\end{figure}

In each plot we draw a linear fit line showing the relationship of unchanged size and simplified size. In the plots where \hl{things don't generalize after simplification (TOP ROW?)}, the slope is considerably higher than in the plots where \hl{things do generalize after simplification}. This indicates that \hl{more simplification is happening with more generalization}, and non-generalizers don't simplify as much. \hl{This paragraph is bad and should be rewritten.}


%\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
%\centering
%\includegraphics[width=\linewidth]{Size_density_by_generalized_count}
%\caption{Size counts of simplified programs. Includes all simplified programs, from both ones that did and did not generalize pre-simplification.}
%\label{fig:count:gen}
%\end{figure}

%These look at the same graphs, but faceted.
%
%\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
%\centering
%\includegraphics[width=\linewidth]{Size_density_by_generalized_A_count}
%\caption{Size counts of simplified programs where pre-simplified program did generalize.}
%\label{fig:count:pre-simp-gen-true}
%\end{figure}
%
%\begin{figure}[t] %[t] sets the image at the top of the page; t = top, b = bottom, h = here%
%\centering
%\includegraphics[width=\linewidth]{Size_density_by_generalized_B_count}
%\caption{Size counts of simplified programs where pre-simplified program did \underline{not} generalize.}
%\label{fig:count:pre-simp-gen-true}
%\end{figure}


\section{Discussion}
\label{sec:discussion}

\todo[inline]{Talk about what the results mean}

%Here are some potentially interesting questions to explore with this data:
%
%    Which simplification method tends to result in the smallest programs on average? Is there any significant differences here?
%
%    Is getting smaller during simplification important for generalization? In other words, do smaller programs post-simplification tend to generalize better than larger ones?
%
%        Note: we could ask this question of all the simplified programs, but we could also concentrate only on runs that had some generalizing programs and some non-generalizing programs, which might give us a more detailed view. It would probably be worthwhile to look into both of these. I'm not sure exactly how we would investigate this. @mcphee made some cool graphs last year that start answering this question. I've included one at the bottom of this post that was made with limited data; a similar graph for the full data might be a good place to start.
%
%    It might be worthwhile to break this big plot into smaller plots, grouping together similar plots. For example, we might group plots based on how often unsimplified programs generalize -- groups of something like 0-50\%, 50-95\%, and 95-100\% generalization. Or, we could do it some other way. But, this could definitely help with spotting differences based on problem.
%

\hl{    Would a multi-restart simplification get us to smaller programs more often than the other methods we've tried? This could potentially avoid unlucky local minima. The backtracking methods are also trying to avoid unlucky local minima, so this would be an interesting comparison. It seems like we could answer this question with the data we have.}

\subsection{Nic's Thoughts on Unused Push Instructions}
Anyway, I wouldn't recommend using the word "bloat" because I also think that is generally associated with a tendency to get bigger and bigger and bigger in ways that aren't connected to improvements in fitness, and that doesn't seem to be a Clojush thing. (I'm personally not super clear on why that's not a thing in Clojush, etc., etc., but that's not today's problem.)

Even more complicated is the fact that in Push programs we often find various categories of "unused" code. There are instructions that do nothing because their arguments aren't on the stack, or sometimes aren't on the stack. There are instructions that do something, but no one cares (they push a boolean that's never used). There are instructions that are there just to add content to a stack, but who's particular action doesn't matter. Etc., etc. These are not things "standard tree-based GP" people run into very often, and might not be obvious to them.

Lastly, it can be really hard to tell what's being used and how in complex programs, and how that relates to the simplified versions. It's easy to implicitly assume that the simplified program is "the thing", and that everything that got removed didn't really matter in the original execution. (This is often an assumption in tree-based bloat discussions.) I'm pretty sure this is often very not true, however, in Push, and that's important to the generalization. It seems fairly common, for example, for Push programs (especially with the "standard" instruction set) to have a lot of unnecessary looping that frequently gets removed or reduced in simplification. Often that looping is actually executed in the original program, so it's part of how the training answers are formed. I have a suspicion that part of why simplification often improves generalization is that it removes these unnecessary loops that are sometimes fragile and do weird things on certain inputs.



\section{Related Work}
\label{sec:related}

\todo[inline]{Tom: I'm fine moving this section earlier if others prefer.}

\todo[inline]{Below are things I've (Tom) come across in the past year potentially realted to this work. We'll have to go through and see what's worth citing.}

\subsection{Papers about automatic simplification}

\begin{itemize}

\item
Genprog minimization after run (see 7/24/16)

\item
Field Guide to Genetic Programming: p 64 top: Bahnzaf paper might be precursor to automatic simplification

\item
This paper in neural networks might be related: GECCO 2016 - Identifying Core Functional Networks and Functional Modules within Artificial Neural Networks via Subsets Regression

\item
Differentiate between our work and algebraic simplification (as used in semantic GP), since this CAN change the semantics on inputs not in the training set, where algebraic methods cannot. Also, algebraic methods wouldn't work on general programs that we're evolving

\item
Algebraic Simplification of GP Programs During Evolution (I think this is an actual paper title)

\item
Investigation of simplification threshold and noise level of input data in numerical simplification of genetic programs \cite{Kinzett:2010:cec}

\item
Smaller networks in neural nets: https://push-language.hampshire.edu/t/gecco-2017-simplification-for-generalization/660/21?u=thelmuth

\end{itemize}

\subsection{Papers about generalization and overfitting in GP}

\begin{itemize}
\item
GECCO 2011 - Variance based Selection to Improve Test Set Performance in Genetic Programming

\item
Controlling overfitting in symbolic regression based on a bias/variance error decomposition


\end{itemize}

\subsection{Other related papers}

\begin{itemize}
\item
Tree-structured differencing: R. Al-Ekram, A. Adma, and O. Baysal. diffX: an algorithm to detect changes in multi-version XML documents. In Conference of the Centre for Advanced Studies on Collaborative research, pages 1–11. IBM Press, 2005.

\item
Delta debugging: A. Zeller. Yesterday, my program worked. Today, it does not. Why? In Foundations of Software Engineering, pages 253–267, 1999.

\end{itemize}

\section{Conclusions and future work}
\label{sec:conclusions}

\todo[inline]{Maybe this could/should be two sections, but I bet we won't have room.}

FUTURE WORK BELOW HERE

Testing automatic simplification on other types of problems. Would it work on symbolic regression? Classification? My (TMH) guess is that it would work better on classification than symbolic regression.

How could simplification be adapted for tree-based GP? How about other GP, such as Cartesian or grammar-based? Grammatical evolution could potentially remove numbers from the genome, or could replace trees in the resulting program. Other linear GPs seem like good targets.

\begin{acks}
  
  \todo[inline]{Put acknowledgements here, including grants.}

\end{acks}
